TODO:

Misc:
- weaveate benchmarks speed and accuracy seperatly. consider this

Databases:
- look into the following
    - SIFT1M
    - Glove-25 and 100
    - Deep image 96
    - GIST 960
- get proof of random vectors being slower for HNSW
- implement even bigger Databases

Models:
- look into which models to benchmark
- implement filtering as well
- think about distance measure. 
- implement wrapper for faiss such that it can be passed vectors. It's wierd to build before hand.

Metrics:
- implment p-99 and p-x latency. maybe get the whole distribution
- recall@1, recall@10, recall@k
- maybe use GCP as benchmarking platform
- use python timeit to avoid garbage collection etc.?

Figure out how to standardize the build task.



Simpliest benchmarks:

scalar quantization (float -> int), for increase in speed
product quantization (subvectors represented as centroids), for increase in memory
index architecture (hsnw, ivf, composite indexes etc.)

so. implement qdrant local with possibility to change
ef
scalar quantization
product quantization

do a large a.f. benchmark. save distributions


Define some pedantic object for storing the index settings
Write a faiss implementation that also creates the index
 / just write a wrapper for the existing one.
Streamline the benchmarking process for HSNW with options for quantization, m and ef.
The search should be able to be built and run concurrently. Remember that.

So the qdrant implementation might be bad. I'll also have to spin up a server everytime I build an index?