# @package _global_

defaults:
  - override /model/encoder: t5-large
  - _self_

resources:
  num_proc: 16 # <--- 16 workers to preprocess data on global rank zero

batch_size:
  effective: 32
  per_device: 4
  per_device_eval: 8
  per_device_predict: 1024

collates:
  # Defines the configurations for `dataset_builder.HfBuilder._collate_config`
  train:
    n_sections: 24
    max_pos_sections: 8
    prefetch_n_sections: 64

trainer:
  precision: bf16-mixed