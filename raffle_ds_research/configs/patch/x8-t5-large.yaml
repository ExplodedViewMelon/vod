# @package _global_

# This config is for 4xA100 GPUs (40GB)

defaults:
  - override /model/encoder: t5-large
  # model name
  - override /trainer/strategy: deepspeed_2
  # DeepSpeed strategy ZeRO-2

resources:
  num_workers: 2
  num_proc: 16
  omp_threads: 32

batch_size:
  effective: 32
  per_device: 4
  per_device_eval: 16
  per_device_predict: 1024

collates:
  # Defines the configurations for `dataset_builder.HfBuilder._collate_config`
  train:
    n_sections: 24
    max_pos_sections: 8
    prefetch_n_sections: 128

fabric:
  precision: bf16-mixed
  accelerator: gpu