# @package _global_

# This config is for 4xA100 GPUs (40GB)

defaults:
  - override /model/encoder: bert-base
  # model name
  - override /trainer/strategy: deepspeed_2
  # DeepSpeed strategy ZeRO-2

resources:
  num_workers: 2
  num_proc: 16
  omp_threads: 32

batch_size:
  effective: 32
  per_device: 8
  per_device_eval: 32
  per_device_predict: 2048

collates:
  # Defines the configurations for `dataset_builder.HfBuilder._collate_config`
  train:
    n_sections: 96
    max_pos_sections: 16
    prefetch_n_sections: 256

fabric:
  precision: bf16-mixed
  accelerator: gpu