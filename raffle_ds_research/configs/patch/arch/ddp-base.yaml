# @package _global_

# This config is for 4xA100 GPUs (40GB)

defaults:
  # HF transformer model name
  - override /model/encoder: bert-base
  # LR scheduler
  - override /model/scheduler: linear
  # Fabric Strategy
  - override /fabric/strategy: ddp

resources:
  num_workers: 2
  num_proc: 16
  omp_threads: ${int_div:${n_cpus:},2}

model:
  encoder:
    model_config:
      dropout: 0 # default: 0.1 - doesn't work well with gradient
  optimizer:
    lr: 3e-5
    weight_decay: 1e-3
    eps: 1e-8
  gradients:
    section_chunk_size: null
  compile_encoder: true
  compile_kwargs:
    dynamic: false
    fullgraph: false

batch_size:
  effective: 32
  per_device: 8
  per_device_eval: 16
  per_device_predict: 1024

collates:
  train:
    n_sections: 24
    max_pos_sections: 8
    prefetch_n_sections: 128
    do_sample: true

fabric:
  precision: bf16-mixed
  accelerator: gpu
