# @package _global_

defaults:
  # boost `ai_trainer` and `production` questions
  # - override /dl_sampler: promote_trusted_sources
  # HF model name
  - override /model/encoder: bert-base
  # Gradients
  - override /model/gradients: kl_div
  # model optimizer
  - override /model/optimizer: adamw
  # use the KL divergence from the model to the binary targets
  - override /fabric/loggers:
      - wandb
  - _self_

seed: 1

dataset:
  train:
    - frank.A.en:train
    - squad.en:train
    - msmarco.en:train
  validation:
    - frank.A.en:val
    - squad.en:val
    - msmarco.en:val
  benchmark:
    - frank.A.en:val
    - frank.B.en:val
    - msmarco.en:val

model:
  optimizer:
    lr: 1e-4
    weight_decay: 1e-3
    eps: 1e-8
  gradients:
    self_supervision_weight: 0.0
    bm25_guidance_weight: 0.0

resources:
  num_workers: 2 # <--- 2 workers per GPU
  num_proc: 8 # <--- 8 workers to preprocess data on global rank zero

batch_size:
  effective: 32
  per_device: 8
  per_device_eval: 8
  per_device_predict: 2048

trainer:
  max_steps: 200_000
  period: 20_000
  val_check_interval: 500
  n_max_eval: 1_000
  pbar_keys: ["kl_data", "hitrate_3"]
  parameters:
    bm25: 1.0
    faiss:
      mode: linear
      start: 0
      value: 1
      period: ${trainer.period}
      offset: ${trainer.period}

benchmark:
  n_max_eval: 1_000
  on_init: false
  tune_parameters: true
  n_tuning_steps: 1_000

collates:
  # Defines the configurations for `dataset_builder.HfBuilder._collate_config`
  train:
    n_sections: 48
    max_pos_sections: 8
    prefetch_n_sections: 64
    do_sample: false
    in_batch_negatives: true
  benchmark:
    prefetch_n_sections: 500
    do_sample: false

search:
  faiss:
    factory: OPQ32,IVFauto,PQ32x8
    metric: inner_product
    nprobe: 32
    train_size: 1_000_000
  bm25:
    text_key: text
    group_key: group_hash
    persistent: true
