# @package _global_

defaults:
  - override /dataset/templates: encode_context
  - override /model/encoder: bert-tiny
  - _self_

seed: 1

model:
  gradients:
    section_chunk_size: null

benchmark:
  n_max_eval: 100
  on_init: false
  tune_parameters: false # <- TODO!
  n_tuning_steps: 100
  parameters:
    bm25: 1.0
    faiss: 0.0
  search:
    faiss:
      factory: Flat

trainer:
  max_steps: 1000
  period: 300
  val_check_interval: 5
  n_max_eval: 100
  parameters:
    bm25: 1.0
    faiss: 0.0

dataset:
  subset_size: null
  filter_unused_sections: false
  train: frank.A.en-pos:train
  validation: frank.A.en-pos:val
  benchmark:
    - frank.A.en-pos:val

batch_size:
  effective: 16
  per_device: 2
  per_device_eval: 2
  per_device_predict: 512

resources:
  num_workers: 1
  num_proc: 4

collates:
  train:
    n_sections: 5
    max_pos_sections: 2
    prefetch_n_sections: 12
    do_sample: false
    in_batch_negatives: true
    post_filter: ensure_match

  benchmark:
    n_sections: 64
    max_pos_sections: 16
    prefetch_n_sections: 128
    do_sample: false
    post_filter: ensure_match

dataloaders:
  benchmark:
    batch_size: 3
