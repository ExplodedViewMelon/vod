# @package _global_

defaults:
  - override /builder: frank
  - override /model/encoder: bert-base
  - override /model/gradients: kl_div
  - override /trainer/strategy: deepspeed
  - override /model/optimizer: adam-cpu
  - _self_

model:
  embedding_size: null
  use_pooler_layer: true
  optimizer:
    lr: 1e-4
    weight_decay: 0
  scheduler:
    total_iters: 50

resources:
  num_workers: 4
  num_proc: 4

batch_size:
  effective: 32
  per_device: 16
  per_device_eval: 8
  per_device_predict: 512

trainer:
  accelerator: gpu
  precision: 16-mixed
  detect_anomaly: false
  max_steps: 100_000
  limit_val_batches: 20
  val_check_interval: ${int_mul:${trainer.accumulate_grad_batches},100}


collate_configs:
  # Defines the configurations for `dataset_builder.HfBuilder._collate_config`
  train:
    n_sections: 20
    max_pos_sections: 5
    prefetch_n_sections: 512
    do_sample: false
    question_max_length: 128
    section_max_length: 200
  eval:
    n_sections: 300
    max_pos_sections: 100
    prefetch_n_sections: 1000
    do_sample: false
    question_max_length: 128
    section_max_length: 200


update_freq: 1_000
faiss_config:
  factory: Flat
  schedule: 0
bm25_config:
  use_labels: true
  schedule: 1