# @package _global_

defaults:
  # boost `ai_trainer` and `production` questions
  # - override /dl_sampler: promote_trusted_sources
  # HF model name
  - override /model/encoder: mt5-base
  # Gradients
  - override /model/gradients: supervised_retrieval
  # model optimizer
  - override /model/optimizer: adamw
  # use the KL divergence from the model to the binary targets
  - override /fabric/loggers:
      - wandb
  - _self_

seed: 1

model:
  optimizer:
    lr: 3e-5
    weight_decay: 1e-3
    eps: 1e-8
  gradients:
    anchor_weight: 1e-3
    self_supervision_weight: 0
    guidance_weight: 1e-2
    guidance: zero
  compile_encoder: true
  compile_kwargs:
    dynamic: false
    fullgraph: false

resources:
  num_workers: 2 # <--- 2 workers per GPU
  num_proc: 8 # <--- 8 workers to preprocess data on global rank zero

batch_size:
  effective: 32
  per_device: 8
  per_device_eval: 8
  per_device_predict: 2048

trainer:
  max_steps: 200_000
  period: 10_000
  val_check_interval: 500
  log_interval: 20
  n_max_eval: 5_000
  pbar_keys: ["kl_data", "hitrate_3", "mrr_10"]
  parameters:
    sparse: 0.5
    dense:
      mode: linear
      start: 0
      value: 1
      offset: ${trainer.period}
      period: ${trainer.period}

benchmark:
  n_max_eval: 1_000
  on_init: false
  parameters:
    sparse: 0.5
    dense: 1.0

collates:
  train:
    n_sections: 24 # sample `x` sections from the candidate `z` sections
    max_pos_sections: 8 # include at max `y` positive sections
    support_size: 100
    prefetch_n_sections: 128 # retrieve `z` candidate sections
    do_sample: true # sample the sections from the candidate sections (randomly)
    in_batch_negatives: false
  benchmark:
    n_sections: null
    support_size: null
    prefetch_n_sections: 512
    do_sample: false
