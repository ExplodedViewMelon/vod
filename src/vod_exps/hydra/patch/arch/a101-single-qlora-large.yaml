# @package _global_

# This config is for ==1xA100 GPUs (80GB)

defaults:
  - a101-single-ckpt-large

model:
  encoder:
    torch_dtype: ${fabric.precision}
    quantization_config:
      _target_: transformers.BitsAndBytesConfig
      load_in_4bit: true
      llm_int8_threshold: 6.0
      llm_int8_has_fp16_weight: false
      bnb_4bit_compute_dtype: ${normalize_dtype:${fabric.precision}}
      bnb_4bit_use_double_quant: true
      bnb_4bit_quant_type: nf4
  optim_config:
    _target_: vod_configs.ModelOptimConfig.parse
    compile: false
    gradient_checkpointing: true
    prepare_for_kbit_training: true
    force_dtype: ${normalize_dtype:${fabric.precision}}
    peft_config:
      peft_type: LORA
      target_modules:
        - "*.attention.self.(query|key|value)$"
        - "*.dense$"
        - "*.SelfAttention.(q|k|v)$"
        - "*.DenseReluDense.(wi_0|wi_1|wo)$"
        - "*.projection$"
      r: 16
      lora_alpha: 16
      lora_dropout: 0.01
      bias: none

